{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unnecessary Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear Regression\n",
    "Automatic feature scalling.\n",
    "\n",
    "Only 1 linear line will be shown in the graph\n",
    "\n",
    "It is not possible to apply here as we have many independent columns and we are predicting only 0 or 1\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "\n",
    "regressor.fit(x_train, y_train)\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "y_pred #no where near to y_test. y_test is totaly 0 and 1\n",
    "array([ 0.58586526,  0.28419806,  0.40510362, -0.10924335,  0.96318307,\n",
    "        0.07345202,  0.95858559,  0.39313105,  0.05953165,  0.31214851,\n",
    "        0.73517447,  0.62396311,  0.24226717,  0.66971236,  0.14083869,\n",
    "        0.20900672,  0.62178526,  0.01946794,  0.05451292,  0.68210964,\n",
    "        0.12067922,  0.82729648,  0.10040169,  0.11108237,  0.61261257,\n",
    "        0.34848533,  0.26246193,  0.18899899,  0.08982799,  0.21432556,\n",
    "        0.8034966 ,  0.07390936,  0.59134722,  0.07726794,  0.5590577 ,\n",
    "        0.81964691,  0.86305658,  0.41856413,  0.8320559 ,  0.32238642,\n",
    "        0.47783429,  0.75322679,  0.15996726,  0.48072088,  0.87332309,\n",
    "        0.28017024,  0.71392379,  0.04600367,  0.74955839,  0.23044429,\n",
    "        0.60484346,  0.18925013,  0.76999713,  0.09651033,  0.7901314 ,\n",
    "        0.6229006 ,  0.62256399,  0.01033446,  0.74420001,  0.06443853,\n",
    "        0.62533132,  0.26003372,  0.53912942,  0.32232191,  0.05946222,\n",
    "       -0.09447638,  0.67480599,  0.1060694 ,  0.71309863,  0.60572566,\n",
    "        0.0511322 ,  0.90239034,  0.12381722,  0.2927919 ,  0.18009077,\n",
    "        0.36526183,  0.10055572,  0.13268926,  0.10852956,  0.15003301,\n",
    "        0.09455739,  0.07970672,  0.0697384 ,  0.05526957,  0.12695083,\n",
    "        1.00308999, -0.00257066,  0.06333738,  0.69125434,  0.45620966,\n",
    "        0.39655292,  0.06339374,  0.2769279 ,  0.11444695,  0.25995978,\n",
    "        0.05442339,  0.70879298, -0.05322717,  0.17902519,  0.31963218,\n",
    "        0.92721354,  0.66990555,  0.11209862,  0.24779856,  0.13901149,\n",
    "        0.31123068,  0.06694298,  0.69726281,  1.04294211,  0.19567297,\n",
    "        0.07123017,  0.07709645, -0.00912692,  0.60313931,  0.10597008,\n",
    "        0.74367686,  0.11238438,  0.69205582,  0.53113117,  0.3322026 ,\n",
    "        0.51864287,  0.08349858,  0.87424759,  0.30589309,  0.18001604,\n",
    "        0.11446095,  0.97147147,  0.40232197,  0.50216123,  0.0824596 ,\n",
    "        0.88689373,  0.10359267,  0.92136249,  0.53695252,  0.14879138,\n",
    "        0.53403086, -0.03403411,  0.14823514,  0.11899778,  0.14863275,\n",
    "        0.40392727,  0.66338106,  0.17465955, -0.01265184,  0.75896695,\n",
    "       -0.0537133 ,  0.25455405, -0.025334  ,  0.03122935,  0.09181106,\n",
    "        0.25705238,  0.47979488,  0.93558999,  0.81219344,  0.85292764,\n",
    "        0.15381984,  0.07689739,  0.57925758,  0.36408646,  0.06779299,\n",
    "        0.69376054,  0.30610543,  0.01068449,  0.11349574,  0.71464281,\n",
    "        0.19923292,  0.14126286,  0.1158851 ,  0.72421981,  0.74670912,\n",
    "        0.24808277,  0.46522808, -0.28247319,  0.01959527,  0.19719026,\n",
    "        0.7003355 ,  0.0417981 ,  0.08669988,  0.84075726,  1.03993414,\n",
    "        0.31076978,  0.11160287,  0.8701556 ,  0.83091886,  0.09396762,\n",
    "        0.91096315,  0.62993075,  0.03004559,  0.87615759,  0.0838168 ,\n",
    "        0.24292085,  0.64599936,  0.74767354, -0.06888329,  0.1231882 ,\n",
    "        0.40547002,  0.95185729,  0.66523048,  0.96457731,  0.62711119,\n",
    "        0.21645405,  0.15920009,  0.06116246,  0.83696815,  1.00920681,\n",
    "        0.64634422,  0.87072094,  0.02635445,  0.16574313,  0.07761544,\n",
    "        0.27118525,  0.21044737,  0.74350633, -0.17390102,  0.67138332,\n",
    "        0.91338425,  0.5992287 ,  0.28018956,  0.21796089,  0.15006163,\n",
    "        0.46420571,  0.10949869,  0.44089557])\n",
    "\n",
    "#plt.scatter(x_test, y_test, color = 'red')\n",
    "plt.plot(x_test, y_test)\n",
    "plt.title('Titanic')\n",
    "plt.xlabel('many columns of x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "Multiple Linear Regression\n",
    "Automatic feature scalling.\n",
    "\n",
    "Multiple Linear Lines will be representated in the graph\n",
    "\n",
    "for every independent variable we need a separate dimention. here we have 12 columns. so it is hard to visually represent 12 dimentions. here I have tried to create a plot with 2 independent variables only.\n",
    "\n",
    "It is not possible to apply Multiple Linear Regression here as we are predicting only 0 or 1. so the graph will be meaningless\n",
    "\n",
    "\n",
    "# Fitting Multiple Linear Regression to the Training set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "\n",
    "regressor.fit(x_train, y_train)\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(x_test) \n",
    "\n",
    "plt.scatter(x_test[:,11], y_test, color = 'red')\n",
    "plt.scatter(x_test[:,8], y_test, color = 'purple')\n",
    "plt.plot(x_test[:,11], y_test, color = 'blue')\n",
    "plt.plot(x_test[:,8], y_test, color = 'green')\n",
    "plt.title('testing')\n",
    "plt.xlabel('x - index 8 & 11')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_pred #no where near to y_test. y_test is totaly 0 and 1\n",
    "array([ 0.24762951,  0.77959101,  0.13368432,  1.03678471,  0.13580791,\n",
    "        0.31878736,  0.04210209,  0.36557768,  0.29436295,  0.25295793,\n",
    "        0.34148707,  0.42321286,  0.28731804,  0.92473743,  0.16583827,\n",
    "        0.8781635 ,  0.15253539,  1.07340179,  0.13560991,  0.55994053,\n",
    "        0.19666398,  0.12090861,  0.08578027,  0.53808034,  0.71540572,\n",
    "        0.53397058,  0.40870246,  0.13961686,  0.51754887,  0.77723642,\n",
    "        0.06267473,  0.14994894,  0.07352216,  0.1442136 ,  0.07308154,\n",
    "        0.07424396,  0.68823563,  0.32500889,  0.64226385,  0.18807713,\n",
    "        0.84151723,  0.11703028,  0.08798203,  0.15636509,  0.33700887,\n",
    "        0.10795107,  0.16345993,  0.08618088,  0.09372623,  0.38256406,\n",
    "        0.55451758,  0.09290282,  0.9769394 ,  0.47883625,  0.06761067,\n",
    "        0.61700191,  0.59051942,  0.25545834,  1.03206791,  0.08922329,\n",
    "        0.29274659,  0.10759875,  0.14064971,  0.59255046,  0.086686  ,\n",
    "        0.03609136,  0.97016255,  0.81293755,  0.13794127, -0.01024756,\n",
    "        0.37593255,  0.08831201,  0.17242536,  0.13622657,  0.13639094,\n",
    "        0.90234439,  0.92050129,  0.4225586 ,  0.86717747,  0.2512886 ,\n",
    "        0.26156306,  0.28333579,  0.49161967,  0.10165801,  0.8424836 ,\n",
    "        0.44303556,  0.52202178,  0.15441838,  0.03291439,  0.43813991,\n",
    "        0.87130537,  0.11767567,  0.43335911, -0.17269494,  0.04246625,\n",
    "        0.16814239,  0.70194977,  0.49733856,  0.07906657,  0.13302536,\n",
    "        0.04971928,  0.14997925,  0.25544168,  0.59145674,  0.05720329,\n",
    "        0.89968752,  0.18773746,  0.02045205,  0.80108143,  0.69123842,\n",
    "        0.68536089,  0.31033566,  0.15990147,  0.34416695,  0.58895799,\n",
    "        0.19654401,  0.05374776,  0.12962999,  0.14270872,  0.24908122,\n",
    "        0.71443561,  0.08885039,  0.81831187,  0.4295046 ,  0.86568773,\n",
    "        0.29437228,  0.07226938,  1.06494857,  0.15516613,  0.19626313,\n",
    "        1.02520841,  0.57070142,  0.60709697,  0.60184284,  0.43061451,\n",
    "        0.22686895,  0.10768616,  0.88222774,  0.06637474,  0.18940297,\n",
    "        0.22894161,  0.7686726 ,  0.16311353,  0.14400131,  0.10087007,\n",
    "        0.09332301,  0.6364189 ,  0.0505897 ,  0.07138135,  0.13849404,\n",
    "       -0.03265763,  0.53486521,  0.32319318,  0.33495962,  0.07485671,\n",
    "        0.12544923,  0.68479383,  0.87927326,  0.15696269,  0.19264195,\n",
    "        0.63810914,  0.14877304,  0.19695486,  0.38609936,  0.34410932,\n",
    "        0.94431593,  0.00460219,  0.93493416,  0.63955293,  0.22248452,\n",
    "        0.01529526,  0.33776373,  0.80620907,  0.20049656,  0.32479923,\n",
    "        0.98555299,  0.4324338 ,  0.75191136,  0.84414842,  0.25720644,\n",
    "        0.50418668,  0.6087517 ,  0.70269676,  0.09114217,  0.96813192,\n",
    "        0.10704001,  0.1576163 ,  0.25001507,  0.97382937,  0.8847759 ,\n",
    "        0.17113222,  0.08831391,  0.11224633,  0.15616286,  0.10192528,\n",
    "        0.15052335,  0.32791297, -0.01066457,  0.94677402,  0.66605881,\n",
    "       -0.02032625,  0.63918251,  0.09040137,  0.27439901,  0.15349597,\n",
    "        0.20157653,  0.61812345,  0.29935986,  0.59102993,  0.12571744,\n",
    "        0.0867028 ,  0.15141628,  0.44356554,  0.02042849,  0.1135807 ,\n",
    "        0.0964156 ,  0.58908629,  0.0418584 ,  0.08836485, -0.01523954,\n",
    "        0.32875486,  0.57564228,  0.41138063])\n",
    "Polynomial Regression\n",
    "Automatic feature scalling\n",
    "\n",
    "Non linear graph\n",
    "\n",
    "It is not possible to apply Polynomial Regression here as we are predicting only 0 or 1. With 0 and 1 only Polynomial line will never come\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_reg = PolynomialFeatures()\n",
    "\n",
    "x_poly = poly_reg.fit_transform(x_train)\n",
    "\n",
    "x_train\n",
    "array([[  1.    ,   0.    ,   0.    , ...,   1.    ,   0.    ,  78.2667],\n",
    "       [  1.    ,   0.    ,   0.    , ...,   0.    ,   0.    ,   7.65  ],\n",
    "       [  0.    ,   1.    ,   0.    , ...,   1.    ,   0.    ,  15.5   ],\n",
    "       ...,\n",
    "       [  0.    ,   1.    ,   0.    , ...,   0.    ,   0.    ,   8.05  ],\n",
    "       [  1.    ,   0.    ,   0.    , ...,   0.    ,   0.    , 153.4625],\n",
    "       [  1.    ,   0.    ,   0.    , ...,   1.    ,   0.    ,  26.    ]])\n",
    "\n",
    "poly_reg.fit(x_poly, y_train)\n",
    "PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)\n",
    "\n",
    "lin_reg_2.predict(poly_reg.fit_transform(6.5))\n",
    "SVR Regression\n",
    "->No Automatic feature scalling. ->need to feature scale y or y_train also. It will give predicted y_test in encoded format. so at last we need to inverse the encoding again for the y_test. ->X and Y must be of same size.\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "\n",
    "x_test = sc_x.fit_transform(x_test)\n",
    "\n",
    "y_train = np.reshape(y_train, (len(y_train),-1))\n",
    "\n",
    "y_train = sc_y.fit_transform(y_train)\n",
    "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
    "  warnings.warn(msg, DataConversionWarning)\n",
    "\n",
    "y_train = np.asarray(y_train).reshape(-1)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "regressor = SVR(kernel = 'rbf')\n",
    "\n",
    "regressor.fit(x_train, y_train)\n",
    "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
    "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "\n",
    "y_pred = regressor.predict(x_test)\n",
    "\n",
    "y_pred = sc_y.inverse_transform(y_pred)\n",
    "\n",
    "y_pred\n",
    "array([ 6.27950313e-01,  1.02506693e+00,  2.30037513e-01,  1.45778516e-02,\n",
    "        1.02138082e+00,  7.32067953e-01, -8.79323395e-04,  4.90358996e-03,\n",
    "        5.91107680e-03,  9.42181922e-03,  9.68637553e-01,  3.73412982e-02,\n",
    "        1.53174188e-01,  2.10039802e-02,  1.75044366e-03,  4.87704593e-03,\n",
    "        1.09937317e-02,  7.03209949e-01,  1.49855529e-01,  1.28843935e-02,\n",
    "        2.50826724e-02,  1.50895832e-02,  2.07805942e-01,  1.74295198e-01,\n",
    "       -6.11360433e-03, -3.87304488e-03,  7.29276386e-01,  6.60625366e-01,\n",
    "        8.53012475e-01,  9.04967938e-01, -9.84183505e-03,  1.56820587e-02,\n",
    "        6.63801874e-02,  1.54515711e-02,  5.48025400e-02, -1.70464873e-02,\n",
    "        9.97463202e-01,  1.59033086e-01,  9.63577263e-01,  3.69368899e-01,\n",
    "        9.48813940e-01,  1.14694772e-02,  6.94479227e-01,  1.54227669e-01,\n",
    "        9.05327538e-01,  1.12768464e-02,  9.58516821e-01,  8.51698262e-01,\n",
    "        7.45981251e-03, -2.43869370e-02,  6.28628320e-02,  9.25878330e-01,\n",
    "        8.03750843e-01,  3.52646216e-02,  2.51481441e-02,  1.41775116e-02,\n",
    "        1.80911724e-02,  1.16201439e-02,  3.68282272e-02,  6.13259624e-03,\n",
    "        5.39086347e-02,  2.34763541e-02,  6.74765547e-01,  9.20328261e-01,\n",
    "       -6.20708777e-03, -1.09200172e-02,  3.51489558e-02, -5.41027261e-02,\n",
    "        4.34038158e-01, -4.64528827e-03,  1.36948071e-02,  7.12018275e-01,\n",
    "        2.34961603e-02,  9.73562388e-01,  3.94873169e-02,  1.59262071e-02,\n",
    "        8.50031411e-03,  1.39432445e-02, -1.85686352e-02,  1.00564800e+00,\n",
    "        6.48243964e-02, -2.40043116e-02, -6.42573557e-03,  2.33287279e-03,\n",
    "        1.68685438e-02,  1.48916126e-02, -5.51548992e-02,  9.42717436e-01,\n",
    "       -3.74212589e-03,  5.79341889e-01,  6.49707222e-01, -1.27821573e-02,\n",
    "        1.56521363e-02,  1.15112563e-02,  2.12019198e-02,  8.68298820e-01,\n",
    "        4.58729316e-04,  1.53254514e-02, -4.76094556e-03,  9.07237594e-01,\n",
    "        9.37773986e-01,  8.71591597e-01, -7.78681673e-03,  9.47688569e-01,\n",
    "       -1.04520442e-02,  2.05043447e-02, -9.28578742e-02, -6.93563134e-03,\n",
    "        4.60233127e-02,  6.90317973e-02,  9.64168911e-01,  6.91588291e-01,\n",
    "        2.35537528e-02,  1.16052704e-02, -9.44479708e-04,  9.91226390e-01,\n",
    "        5.07645924e-02,  1.00902687e+00,  8.65942097e-01, -2.63921315e-02,\n",
    "       -3.58485633e-02,  1.59331425e-02, -2.22980747e-02,  4.79550161e-01,\n",
    "       -8.57061794e-03,  6.57492350e-01,  9.20321492e-01,  1.04144280e-01,\n",
    "        1.74798404e-02,  9.92524773e-04,  5.59883715e-01,  1.81758867e-02,\n",
    "       -1.28183238e-02,  1.09344455e-02,  9.67722303e-01,  9.51380132e-01,\n",
    "        4.99667427e-02,  1.02397280e+00,  9.51456220e-01, -4.28447770e-03,\n",
    "        9.27651475e-01,  4.04303471e-01,  6.66907698e-01,  1.18489675e-02,\n",
    "       -3.54562062e-02, -1.63230000e-02,  7.29142533e-01,  1.00876584e+00,\n",
    "        1.81977267e-02, -8.50862345e-04,  9.63311184e-01,  9.21565623e-01,\n",
    "        4.50332979e-02,  7.39216782e-01,  2.31463718e-02,  1.15403697e-02,\n",
    "       -3.46948729e-02,  1.02838006e+00,  2.82641572e-02,  6.27699911e-03,\n",
    "        8.77097519e-01,  7.19515235e-01,  1.35819660e-02,  8.42375178e-01,\n",
    "       -7.88875460e-04, -7.43475445e-03, -2.80651116e-02,  1.00420975e+00,\n",
    "        9.05245636e-01,  4.20884042e-02,  1.01442198e+00,  8.57984059e-01,\n",
    "        6.00799871e-02,  9.26588912e-02,  9.63397432e-01,  7.31729114e-01,\n",
    "        8.51293770e-01,  9.27706744e-01,  9.24010624e-01,  1.17828874e-02,\n",
    "        9.23490760e-01,  9.89106941e-01,  5.22643477e-01,  1.07147941e-01,\n",
    "        2.39512220e-02,  1.05777333e-01,  5.99707033e-01,  2.30357487e-01,\n",
    "        6.66112403e-01,  2.68992772e-02,  9.50140148e-01,  9.50168334e-02,\n",
    "        9.51325387e-01, -2.02947791e-03,  9.71189585e-01,  7.83791312e-02,\n",
    "        8.38653918e-01,  7.45974438e-01,  9.45899860e-01,  4.32264148e-02,\n",
    "        2.02275002e-02,  7.21558208e-01,  9.40205963e-01,  1.22379042e-02,\n",
    "        1.15353083e-02,  2.05950763e-01,  1.73579833e-02,  7.42283455e-02,\n",
    "        2.08717323e-02,  2.22115413e-02,  2.95993293e-02,  1.25269412e-02,\n",
    "        3.69262118e-02,  1.40703763e-02,  1.19640618e-01,  9.37821040e-01,\n",
    "        6.61854584e-01,  9.13523692e-01,  7.66220000e-01,  9.32901658e-01,\n",
    "        4.08281612e-01,  9.80632191e-01,  3.90034517e-01])\n",
    "\n",
    "y_test\n",
    "array([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
    "       1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
    "       1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
    "       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
    "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
    "       0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
    "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
    "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
    "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
    "       1, 1, 0], dtype=int64)\n",
    "\n",
    "plt.plot(x_test, y_test)\n",
    "[<matplotlib.lines.Line2D at 0x248dceb5eb8>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec3048>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec3198>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec32e8>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec3438>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec3588>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec36d8>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec3828>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec3978>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec3ac8>,\n",
    " <matplotlib.lines.Line2D at 0x248dcbb9be0>,\n",
    " <matplotlib.lines.Line2D at 0x248dcec3d30>]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-cad119851a31>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-cad119851a31>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    conda install py-xgboost\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "conda install py-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-477fa34615c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
